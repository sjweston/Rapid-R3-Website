mean(x)
-1/.8165
7-(1.96*0.8165)
7+(1.96*0.8165)
x = c(400, 410, 370, 410, 380, 420, 410, 390, 390, 430)
length(x)
mean(x)
x-362
(x-362)^2
sum((x-362)^2)
sum((x-362)^2)/9
sqrt(sum((x-362)^2)/9)
chavez = c(17,17,19,13,8,20,16,19,18,18,12)
mean(chavez)
chavez - 16.09
dev = chavez - 16.09
dev[11] = .75
dev^2
sum(dev^2)
25+29.5
54.5/66
chavez-16.09
(chavez-16.09)^2
sum((chavez-16.09)^2)
24+27
scores = c(51, 52.5, 54.5)
scores/66
3.5-6
-2.5/2.24
exercise = c(5,12,8,12,9)
happy = c(1,4,3,4,3)
mean(exercise)
sd(exercise)
mean(happy)
sd(happy)
plot(exercise, happy)
happy = c(1,3,4,7,7,8)
exercise = c(6,21,10,10,19,30)
cor(happy,exercise)
scale(exerise)
scale(exercise)
scale(exercise)*scale(happy)
x = scale(exercise)*scale(happy)
x
x[[1]]
dim(x)
mean(x)
s1 = 1200
s2 = 1100
n1 = 15
n2 = 15
((s1^2)*(n1-1))+((s2^2)*(n2-1))
(((s1^2)*(n1-1))+((s2^2)*(n2-1)))/(n1+n2-2)
sqrt((((s1^2)*(n1-1))+((s2^2)*(n2-1)))/(n1+n2-2))
spool = sqrt((((s1^2)*(n1-1))+((s2^2)*(n2-1)))/(n1+n2-2))
spool^2*(30/(15x15))
spool^2*(30/(15*15))
sqrt(spool^2*(30/(15*15)))
(s1-s2)/spool
m1 = 3880.60
m2 = 2900.40
(m1-m2)/spool
26.48*(12/36)
sqrt(8.83)
8/2.97
8-(2.97*2.447)
8+(2.97*2.447)
8-(2.97*2.228)
8+(2.97*2.228)
1200^2
1200^2+(1100^2)
(1200^2)+(1100^2)
((1200^2)+(1100^2))/2
var = ((1200^2)+(1100^2))/2
var(30/(15*15))
var*(30/(15*15))
sqrt(var*(30/(15*15)))
3880-2900
(3880-2900)/420.32
980/sqrt(1325000)
122+16+22+2
13+17+42+12
29+8+21+1
3+6+41+25
162+84+59+75
m1 = c(15,10,7,18,5,9,12)
m2 = c(24,23,11,25,14,14,21)
d = m2-m1
d
d^2
mean(d)
d-mean(d)
(d-mean(d))^2
sum((d-mean(d))^2)
54/6
3/sqrt(7)
mean(m1)
mean(m2)
8/1.13
2.447*1.13
8-2.77
sd(m2)
sd(m1)
m1-m2
mean(m1-m2)
mean(m1)-mean(m2)
x = c(6,21,10,10,19,30)
mean(x)
sd(x)
(x-mean(x))/8.97
zx = (x-mean(x))/8.97
y = c(1,3,4,7,7,8)
mean(y)
sd(y)
zy = (y-mean(y))/2.76
zy
zx*zy
round(zx,2)*round(zy,2)
mean(round(zx,2)*round(zy,2))
scale(x)
round(scale(x),2)
round(scale(x),2)*round(scale(y),2)
mean(round(scale(x),2)*round(scale(y),2))
mean(scale(x)*scale(y))
cor(x,y)
(x-mean(x))^2
sum((x-mean(x))^2)
sum((x-mean(x))^2)/6
sqrt(sum((x-mean(x))^2)/6)
s_x
s_x=sqrt(sum((x-mean(x))^2)/6)
s_7=sqrt(sum((y-mean(y))^2)/6)
s_y=sqrt(sum((y-mean(y))^2)/6)
zx = (x-mean(x))/s_x
zy = (y-mean(y))/s_y
zx*zy
mean(zx*zy)
s_x
sy
s_y
z_x
zx
round(zx,2)
round(zx,2)*round(zy,2)
mean(round(zx,2)*round(zy,2))
round(zy,2)
round(round(zx,2)*round(zy,2),2)
mean(round(round(zx,2)*round(zy,2),2))
x = c(3,4,1,1)
ss = x*x
ss
ss = sum(x*x)
ss
s = ss/3
s = sqrt(ss/3)
s
3/sqrt(3)
data.frame(x = c(1,1,1,1,2,2,2,2), y = c(8,8,8,6,5,7,9,2))
t.test(y~x, var.equal=T)
data =data.frame(x = c(1,1,1,1,2,2,2,2), y = c(8,8,8,6,5,7,9,2))
t.test(y~x, data=data, var.equal=T)
mod = t.test(y~x, data=data, var.equal=T)
str(mod)
x1 = c(8,8,8,6)
x2 = c(5,7,9,2)
sd(x1)
sd(x2)
s1 = sd(x1)
s2 = sd(x2)
data.frame(x = rep(c(1,2), each = 4), y = c(x1,x2))
data = data.frame(x = rep(c(1,2), each = 4), y = c(x1,x2))
library(MVR)
install.packages("MVR")
library(lsr)
lsr::independentSamplesTTest(formula=y~x, data = data)
lsr::independentSamplesTTest(formula=y~x, data = data, var.equal=TRUE)
test = lsr::independentSamplesTTest(formula=y~x, data = data, var.equal=TRUE)
str(test)
den = s1(3) + s2(3)
den = s1*3 + s2*3
num = 6
den/num
s = den/num
s/sqrt(8)
library(psych)
describe(bfi, fast = T)
plot(bfi$N1, bfi$E3)
plot(bfi$N1, bfi$E3, jitter = T)
library(lavaan)
lavaan:::lav_fit_measures
filepathdata <- "~/Downloads/"
load(paste0(filepathdata, "fullSample.Rdata"))
load("/Volumes/NO NAME/fullsample.Rdata")
suppressPackageStartupMessages(library(zipcode))#
suppressPackageStartupMessages(library(mgcv))#
suppressPackageStartupMessages(library(maps))
gridsize = 1000 # default is 300 # 1000 is the best value (lots of testing)#
#
sapaZip <- #
function (data = data, gridsize = gridsize, DV = DV, database = database, #
    average = average, regions = ".", size = size, miss = miss, #
    w = NULL, ncols = NULL, main = "", mapdatabase = mapdatabase) #
{#
    zipData <- sapaZipPrep(data)#
    grid <- sapaGrid(data = zipData, gridsize = gridsize)#
    smooth <- sapaSmooth(data = grid)#
    inside <- sapaInside(data = smooth)#
    sapaImage(data = inside, main = main)#
    invisible(inside)#
}#
#
sapaZipPrep <-#
function (data) #
{#
    zipData <- subset(data, select = c(zip, ZCTA))#
    zip <- zipData[, 1]#
    ZCTA <- zipData[, 2]#
    ZCTAn <- 1 - is.na(ZCTA)#
    zipn <- 1 - is.na(zip)#
    zipframe <- data.frame(zip, ZCTA, ZCTAn, zipn)#
    valid <- function(x) {#
        sum(!is.na(x))#
    }#
    if (!require(zipcode)) {#
        stop("The package zipcode must be installed:")#
    }#
    data(zipcode)#
    zips <- zipcode#
    zips$zip <- as.numeric(zips$zip)#
    z <- zipframe[, "zip"]#
    cnames <- colnames(zipframe)#
    for (i in 1:ncol(zipframe)) {#
        if (is.factor(zipframe[, i]) || is.logical(zipframe[, #
            i])) {#
            zipframe[, i] <- as.numeric(zipframe[, i])#
        }#
    }#
    xvals <- list()#
    xvals$mean <- t(matrix(unlist(by(zipframe, z, colMeans, na.rm = TRUE)), #
        nrow = ncol(zipframe)))#
    xvals$n <- t(matrix(unlist(by(zipframe, z, function(x) sapply(x, #
        valid))), nrow = ncol(zipframe)))#
    colnames(xvals$mean) <- colnames(zipframe)#
    colnames(xvals$n) <- paste(colnames(xvals$mean), ".n", sep = "")#
    rownames(xvals$mean) <- rownames(xvals$n) <- xvals$mean[, #
        "zip"]#
    sapa.values <- cbind(xvals$mean, xvals$n)#
    sapa.zip <- merge(sapa.values, zips, by = "zip")#
    return(sapa.zip)#
}#
#
sapaGrid <- #
function (data = data, gridsize = 600, DV = "ZCTAn", database = "usa", #
    average = FALSE, regions = ".") #
{#
    map <- map(database = database, plot = FALSE, regions = regions)#
    if (average) {#
        aver = mean(data[, DV], na.rm = TRUE)#
    }#
    else {#
        aver = 0#
    }#
    data[, DV] <- data[, DV] - aver#
    mat <- matrix(0, gridsize, gridsize)#
    count <- matrix(0, gridsize, gridsize)#
    nsubs <- nrow(data)#
    rangex <- map$range[2] - map$range[1]#
    rangey <- map$range[4] - map$range[3]#
    data["longitude"] <- round((data["longitude"] - map$range[1]) * #
        gridsize/rangex)#
    data["latitude"] <- round((data["latitude"] - map$range[3]) * #
        gridsize/rangey)#
    clean <- subset(data, (data[, "longitude"] > 0))#
    clean <- subset(clean, (clean[, "longitude"] < (gridsize + #
        1)))#
    clean <- subset(clean, (clean[, "latitude"] < (gridsize + #
        1)))#
    clean <- subset(clean, (clean[, "latitude"] > 0))#
    nsubs <- nrow(clean)#
    DVn <- paste(DV, ".n", sep = "")#
    for (observation in (1:nsubs)) {#
        long <- clean[observation, "longitude"]#
        lat <- clean[observation, "latitude"]#
        if (!is.na(clean[observation, DV])) {#
            mat[long, lat] <- mat[long, lat] + clean[observation, #
                DV] * clean[observation, DVn]#
            count[long, lat] <- count[long, lat] + clean[observation, #
                DVn]#
        }#
    }#
    av <- mat/count#
    if (average) {#
        av[!is.finite(av)] <- 0#
    }#
    else {#
        av[!is.finite(av)] <- aver#
    }#
    return(list(average = av, total = mat, count = count))#
}#
#
sapaSmooth <- #
function (data = data, size = 11, miss = 0.05, w = NULL) #
{#
    n <- data$count#
    n[n == 0] <- miss#
    tot <- data$tot#
    nvar <- ncol(tot)#
    if (is.null(w)) {#
        w <- matrix(1, size, size)#
        center <- median(1:size)#
        for (i in 1:size) {#
            for (j in 1:size) {#
                w[i, j] <- (sqrt((i - center)^2 + (j - center)^2))/2#
                w[i, j] <- dnorm(w[i, j])#
            }#
        }#
    }#
    pad <- center - 1#
    padding <- matrix(0, nvar, pad)#
    tot1 <- cbind(padding, tot, padding)#
    n1 <- cbind(padding, n, padding)#
    padding <- matrix(0, pad, ncol(tot1))#
    tot1 <- rbind(padding, tot1, padding)#
    n1 <- rbind(padding, n1, padding)#
    for (i in (pad + 1):(nvar + pad)) {#
        for (j in (pad + 1):(nvar + pad)) {#
            tot[(i - pad), (j - pad)] <- sum(tot1[(i - pad):(i + #
                pad), (j - pad):(j + pad)] * w)/sum(n1[(i - pad):(i + #
                pad), (j - pad):(j + pad)] * w)#
        }#
    }#
    return(tot)#
}#
#
sapaInside <- #
function (data, database = "usa", regions = ".") #
{#
    if (!require(maps)) {#
        stop("The package maps must be installed:")#
    }#
    if (!require(mgcv)) {#
        stop("The package mgcv must be installed")#
    }#
    nlat = nrow(data)#
    nlong = ncol(data)#
    us <- map(database = database, plot = FALSE, regions = regions)#
    bnd <- data.frame(x = us$x, y = us$y)#
    sapa <- matrix(0, nlat * nlong, 3)#
    x <- (rep(1:nlat, each = nlong)/nlong) * (us$range[2] - us$range[1]) + #
        us$range[1]#
    y <- (rep(1:nlong, nlat)/nlat) * (us$range[4] - us$range[3]) + #
        us$range[3]#
    z <- as.vector(t(data))#
    is.in <- inSide(bnd, x, y)#
    x <- x[is.in]#
    y <- y[is.in]#
    z <- z[is.in]#
    n.in <- length(x)#
    zgrid <- matrix(NA, nlat, nlong)#
    for (location in 1:n.in) {#
        lat <- round((y[location] - us$range[3]) * nlat/(us$range[4] - #
            us$range[3]))#
        long <- round((x[location] - us$range[1]) * nlong/(us$range[2] - #
            us$range[1]))#
        zgrid[long, lat] <- z[location]#
    }#
    newgrid <- list(x = (1:nlong) * (us$range[2] - us$range[1])/nlong + #
        us$range[1], y = (1:nlat) * (us$range[4] - us$range[3])/nlat + #
        us$range[3], z = zgrid)#
    return(list(x = x, y = y, z = z, grid = newgrid))#
}#
#
sapaImage <-#
function (data = data, ncols = NULL, main = main, mapdatabase = "state", #
    regions = ".") #
{#
    if (is.list(data)) #
        grid <- data$grid#
    map <- map(database = mapdatabase, plot = FALSE)#
    rangex <- map$range[2] - map$range[1]#
    rangey <- map$range[4] - map$range[3]#
    if (is.null(ncols)) #
        ncols <- ncol(data$z)#
    gr <- colorRampPalette(c("white", "white", "blue"))#
    colramp <- gr(ncols)#
    image(grid, col = colramp, axes = FALSE, main = main)#
    map <- map(mapdatabase, add = TRUE, regions = regions)#
}#
#
png(file=paste(filepathdata, "USmap.png", sep = ""), width=800, height=580)#
sapaZip(data = fullSample, gridsize = gridsize, DV = DV, database = database, average = average, regions = ".", size = 8, miss = 0.03, w = NULL, ncols = NULL, main = "", mapdatabase = mapdatabase)#
dev.off()
sapaZip(data = fullSample, gridsize = gridsize, DV = DV, database = database, average = average, regions = ".", size = 8, miss = 0.03, w = NULL, ncols = NULL, main = "", mapdatabase = mapdatabase)
library(here)#
library(tidyverse)#
library(lavaan)#
library(semTools)
load(here("Google Drive/Work/Research/ongoing/norming/objects/cleaned.Rdata"))
source(here("Google Drive/Work/Research/ongoing/norming/functions/non_empty_cols.R"))
source(here("Google Drive/Work/Research/ongoing/norming/functions/absolute.R"))
source(here("Google Drive/Work/Research/ongoing/norming/functions/mcdonald.R"))
#load superKey file and select just SPI_81 scales#
key = read_csv(here("Google Drive/Work/Research/ongoing/norming/data/superKey.csv")) %>%#
  select(X1, contains("81")) %>%#
  select(1:6) #only the big five
data = data %>%#
  mutate(paid = ifelse(sample == "sapa", "Voluntary", "Paid"),#
         paid_amount = comp,#
         representative = case_when(#
           sample %in% c("escs", "johnson", "psych") ~ "Not representative",#
           sample %in% c("qualtrics", "yougov", "mturk_low", "mturk_med", "mturk_high") ~ "Representative",#
           TRUE ~ NA_character_),#
         sample_m_jj = #
           case_when(sample == "johnson" ~ NA_character_,#
                     TRUE ~ sample),#
         sample_m_jj_escs = case_when(#
           sample %in% c("escs", "johnson") ~ NA_character_,#
           TRUE ~ sample)#
  )#
#
#remove beginning of scale names#
scale_names = gsub("SPI_81_27_5_", "", names(key)[-1])#
#create factor variable (for ordering tables and such)#
scale_names_f = factor(scale_names, levels = scale_names)#
#
comparisons = c("paid", "paid_amount", "representative", "sample", "sample_m_jj", "sample_m_jj_escs")
source(here("Google Drive/Work/Research/ongoing/norming/scripts/fun_measurement-model.R"))
key = key %>%#
  rename(item = X1) %>%#
  gather(key = "scale", value = "value", -item) %>%#
  filter(value != 0) %>%#
  group_by(scale) %>%#
  nest() %>%#
  mutate(scale = gsub("SPI_81_27_5_", "", scale)) %>%#
  mutate(items = map(data, 1)) %>%#
  mutate(scale_key = map(data, 2)) %>%#
  select(-data)
psych::describe(data, fast = T)
mi_data = expand.grid(RID = data$RID, #
                      scale = scale_names, #
                      comparisons = comparisons, #
                      stringsAsFactors = F) %>%#
  full_join(data) %>%#
  group_by(scale, comparisons) %>%#
  nest() %>%#
  full_join(key, by = "scale") %>%#
  #select just scale items and comparison groups#
  mutate(data = map2(.x = data, #
                     .y = items, .f = function(x,y) select(x, y, comparisons))) %>% #
  # #rename items q_1, q_2, etc#
  # mutate(data = map2(.x = data, .y = items, .f = function(x,y) rename_at(x, vars(matches("[0-9]")), #
  #                                    funs(str_replace(., "[0-9]{1,}", as.character(1:length(y))))))) %>%#
  #rename demographic variable as "groups" to indicate in measurement model which groups to assess#
  mutate(data = map2(.x = data, .y = comparisons, .f = function(x,y) rename_at(x, vars(matches(y)),#
                                                                        funs(str_replace(., y, "groups"))))) %>%#
  #reverse code negatively keyed items#
  mutate(data = map2(.x = data, .y = scale_key, #
                     .f = function(x,y) mutate_at(x, .vars = which(y == -1), #
                                                  .funs = function(i) 100-i))) %>%#
  #figure out which items to keep -- these are items that have at least one response in each group#
  mutate(keep = map(data, non_empty_cols)) %>%#
  mutate(keep = map(keep, ~Reduce(intersect, .x))) %>%#
  mutate(nitems = map(keep, length)) %>%#
  ilter(nitems >=2) %>%#
  mutate(data = map2(data, keep, ~select(.x, all_of(.y), groups))) %>%#
  # create measurement model code#
  mutate(model = map(keep, mm))
mi_data = expand.grid(RID = data$RID, #
                      scale = scale_names, #
                      comparisons = comparisons, #
                      stringsAsFactors = F) %>%#
  full_join(data) %>%#
  group_by(scale, comparisons) %>%#
  nest() %>%#
  full_join(key, by = "scale") %>%#
  #select just scale items and comparison groups#
  mutate(data = map2(.x = data, #
                     .y = items, .f = function(x,y) select(x, y, comparisons))) %>% #
  # #rename items q_1, q_2, etc#
  # mutate(data = map2(.x = data, .y = items, .f = function(x,y) rename_at(x, vars(matches("[0-9]")), #
  #                                    funs(str_replace(., "[0-9]{1,}", as.character(1:length(y))))))) %>%#
  #rename demographic variable as "groups" to indicate in measurement model which groups to assess#
  mutate(data = map2(.x = data, .y = comparisons, .f = function(x,y) rename_at(x, vars(matches(y)),#
                                                                        funs(str_replace(., y, "groups"))))) %>%#
  #reverse code negatively keyed items#
  mutate(data = map2(.x = data, .y = scale_key, #
                     .f = function(x,y) mutate_at(x, .vars = which(y == -1), #
                                                  .funs = function(i) 100-i))) %>%#
  #figure out which items to keep -- these are items that have at least one response in each group#
  mutate(keep = map(data, non_empty_cols)) %>%#
  mutate(keep = map(keep, ~Reduce(intersect, .x))) %>%#
  # how many items left per group?#
  mutate(nitems = map(keep, length)) %>%#
  # if fewer than 2 items, can't build measurement model#
  filter(nitems >=2) %>%#
  mutate(data = map2(data, keep, ~select(.x, all_of(.y), groups))) %>%#
  # create measurement model code#
  mutate(model = map(keep, mm))
# two groups analysis#
mi_data = mi_data %>%#
  mutate(model_output = map2(model, data, lavaan::cfa, missing = "FIML")) #
  # run measurement invariance #
mi_data = mi_data %>%#
  mutate(config_mi_output = map2(model, data, measEq.syntax, group = "groups", #
                                 return.fit = T, missing = "FIML"))
mi_data = mi_data %>%#
  mutate(metric_mi_output = map2(model, data, measEq.syntax, group = "groups", #
                                 return.fit = T, missing = "FIML",#
                                 group.equal = "loadings")) #
mi_data = mi_data %>%#
  mutate(scalar_mi_output = map2(model, data, measEq.syntax, group = "groups", #
                                 return.fit = T, missing = "FIML",#
                                 group.equal = c("loadings", "intercepts"))) #
mi_data = mi_data %>%#
  mutate(strict_mi_output = map2(model, data, measEq.syntax, group = "groups", #
                                 return.fit = T, missing = "FIML",#
                                 group.equal = c("loadings", "intercepts", "residuals")))#
  #compare fits#
mi_data = mi_data %>%#
  mutate(compare_config = map2(model_output, config_mi_output, compareFit)) %>%#
  mutate(compare_metric = map2(config_mi_output, metric_mi_output, compareFit)) %>%#
  mutate(compare_scalar = map2(metric_mi_output, scalar_mi_output, compareFit)) %>%#
  mutate(compare_strict = map2(scalar_mi_output, strict_mi_output, compareFit))
fitmeasures = mi_data %>%#
  mutate(model1_gammahat = map(model_output, moreFitIndices, fit.measures = "gammaHat"),#
         model_config_gammahat = map(config_mi_output, moreFitIndices, fit.measures = "gammaHat"),#
         model_metric_gammahat = map(metric_mi_output, moreFitIndices, fit.measures = "gammaHat"),#
         model_scalar_gammahat = map(scalar_mi_output, moreFitIndices, fit.measures = "gammaHat"),#
         model_strict_gammahat = map(strict_mi_output, moreFitIndices, fit.measures = "gammaHat"))#
fitmeasures = fitmeasures %>%#
  mutate(model1_nci = map(model_output, Mc),#
         model_config_nci = map(config_mi_output, Mc),#
         model_metric_nci = map(metric_mi_output, Mc),#
         model_scalar_nci = map(scalar_mi_output, Mc),#
         model_strict_nci = map(strict_mi_output, Mc)) #
fitmeasures = fitmeasures %>%#
  mutate(config_fit = map(compare_config, "fit"),#
         config_cfi = map(config_fit, "cfi"),#
         config_cfi = map(config_cfi, abs_d),#
         config_nci = map2(model1_nci, model_config_nci, function(x,y) abs(x-y)),#
         config_gammahat = map2(model1_gammahat, model_config_gammahat, function(x,y) abs(x-y)))#
fitmeasures = fitmeasures %>%#
  mutate(metric_fit = map(compare_metric, "fit"),#
         metric_cfi = map(metric_fit, "cfi"),#
         metric_cfi = map(metric_cfi, abs_d),#
         metric_nci = map2(model_config_nci, model_metric_nci, function(x,y) abs(x-y)),#
         metric_gammahat = map2(model_config_gammahat, model_metric_gammahat, function(x,y) abs(x-y))) #
fitmeasures = fitmeasures %>%#
  mutate(scalar_fit = map(compare_scalar, "fit"),#
         scalar_cfi = map(scalar_fit, "cfi"),#
         scalar_cfi = map(scalar_cfi, abs_d),#
         scalar_nci = map2(model_metric_nci, model_scalar_nci, function(x,y) abs(x-y)),#
         scalar_gammahat = map2(model_metric_gammahat, model_scalar_gammahat, function(x,y) abs(x-y))) #
fitmeasures = fitmeasures %>%#
  mutate(strict_fit = map(compare_strict, "fit"),#
         strict_cfi = map(strict_fit, "cfi"),#
         strict_cfi = map(strict_cfi, abs_d),#
         strict_nci = map2(model_scalar_nci, model_strict_nci, function(x,y) abs(x-y)),#
         strict_gammahat = map2(model_scalar_gammahat, model_strict_gammahat, function(x,y) abs(x-y)))
fitmeasures = fitmeasures %>%#
  select(-data, -contains("output"), -contains("compare"))
save(mi_data, file = here("Google Drive/Work/Research/ongoing/norming/objects/hidden_obj/modelfit_collection.Rdata"))
save(fitmeasures, file = here("Google Drive/Work/Research/ongoing/norming/objects/fitstatistics_collection.Rdata"))
library(here)#
library(tidyverse)#
library(lordif)#
#
set.seed(20200406)
load(here("Google Drive/Work/Research/ongoing/norming/objects/cleaned.Rdata"))
#load superKey file and select just SPI_81 scales#
key = read_csv(here("Google Drive/Work/Research/ongoing/norming/data/superKey.csv")) %>%#
  select(X1, contains("81")) %>%#
  select(1:6) #only the big five
#remove beginning of scale names#
scale_names = gsub("SPI_81_27_5_", "", names(key)[-1])#
#create factor variable (for ordering tables and such)#
scale_names_f = factor(scale_names, levels = scale_names)#
#
#demographic characteristics for measurement invariance#
demo = c("age", "sex", "education")#
#
#create age bands from 18-65#
data$age = cut(data$age, breaks = c(18, 28, 38, 48, 58, 65), include.lowest = T)#
#
#identify items for each scale and store in a data frame#
key = key %>%#
  rename(item = X1) %>%#
  gather(key = "scale", value = "value", -item) %>%#
  filter(value != 0) %>%#
  group_by(scale) %>%#
  nest() %>%#
  ungroup() %>%#
  mutate(scale = gsub("SPI_81_27_5_", "", scale)) %>%#
  mutate(items = map(data, 1)) %>%#
  mutate(scale_key = map(data, 2)) %>%#
  select(-data)
# create a dataframe where each row refers to a single scale. columns include the name of the scale and a data frame [wherein items are reverse scored when appropriate]#
dif_data = expand.grid(RID = data$RID, #
                      scale = scale_names, #
                      demo = demo, #
                      stringsAsFactors = F) %>%#
  full_join(data) %>%#
  group_by(scale, demo) %>%#
  nest() %>%#
  full_join(key, by = "scale") %>%#
  #select just scale items and demographics#
  mutate(data = map2(.x = data, #
                     .y = items, .f = function(x,y) select(x, y, demo))) %>% #
  # #rename items q_1, q_2, etc#
  # mutate(data = map2(.x = data, .y = items, .f = function(x,y) rename_at(x, vars(matches("[0-9]")), #
  #                                    funs(str_replace(., "[0-9]{1,}", as.character(1:length(y))))))) %>%#
  #rename demographic variable as "groups" to indicate in measurement model which groups to assess#
  mutate(data = map2(.x = data, .y = demo, .f = function(x,y) rename_at(x, vars(matches(y)),#
                                                                        funs(str_replace(., y, "groups"))))) %>%#
  # remove rows from data object that are missing in the groups object #
  mutate(data =  map(data, ~filter(., !is.na(groups)))) %>%#
  #extract column from data list into its own list#
  mutate(groups = map(data, ~select(., matches("groups")))) %>%#
  mutate(groups = map(groups, unlist, use.names=F)) %>%#
  #reverse code negatively keyed items#
  mutate(data = map2(.x = data, .y = scale_key, #
                     .f = function(x,y) mutate_at(x, .vars = which(y == -1), #
                                                  .f = function(i) 100-i))) %>%#
  #select just the items#
  mutate(data = map(data, ~select(., matches("q_"))))
lordif(as.data.frame(dif_data$data[[1]]), group = dif_data$groups[[1]], criterion = "Beta")
dif_data = dif_data %>%#
  mutate(dif = map2(data, groups, .f = function(x,y) lordif(as.data.frame(x), group = y, criterion = "Beta")))
data = data %>% filter(sex != "Other")
dif_data = expand.grid(RID = data$RID, #
                      scale = scale_names, #
                      demo = demo, #
                      stringsAsFactors = F) %>%#
  full_join(data) %>%#
  group_by(scale, demo) %>%#
  nest() %>%#
  full_join(key, by = "scale") %>%#
  #select just scale items and demographics#
  mutate(data = map2(.x = data, #
                     .y = items, .f = function(x,y) select(x, y, demo))) %>% #
  # #rename items q_1, q_2, etc#
  # mutate(data = map2(.x = data, .y = items, .f = function(x,y) rename_at(x, vars(matches("[0-9]")), #
  #                                    funs(str_replace(., "[0-9]{1,}", as.character(1:length(y))))))) %>%#
  #rename demographic variable as "groups" to indicate in measurement model which groups to assess#
  mutate(data = map2(.x = data, .y = demo, .f = function(x,y) rename_at(x, vars(matches(y)),#
                                                                        funs(str_replace(., y, "groups"))))) %>%#
  # remove rows from data object that are missing in the groups object #
  mutate(data =  map(data, ~filter(., !is.na(groups)))) %>%#
  #extract column from data list into its own list#
  mutate(groups = map(data, ~select(., matches("groups")))) %>%#
  mutate(groups = map(groups, unlist, use.names=F)) %>%#
  #reverse code negatively keyed items#
  mutate(data = map2(.x = data, .y = scale_key, #
                     .f = function(x,y) mutate_at(x, .vars = which(y == -1), #
                                                  .f = function(i) 100-i))) %>%#
  #select just the items#
  mutate(data = map(data, ~select(., matches("q_"))))#
#
dif_data = dif_data %>%#
  mutate(dif = map2(data, groups, .f = function(x,y) lordif(as.data.frame(x), group = y, criterion = "Beta")))
which(dif_data$scale == "Extra")
which(dif_data$demo == "Age")
which(dif_data$demo == "age")
lordif(as.data.frame(dif_data$data[[3]]), group = dif_data$groups[[3]], criterion = "Beta")
dif_data$items[[3]]
names(dif_data$data[[3]])
cor(dif_data$data[[3]], use = )
cor(dif_data$data[[3]], use = "pairwise")
psych::describeBy(dif_data$data[[3]], group=dif_data$groups[[3]], fast = T)
psych::alpha(dif_data$data[[3]])
data %>%
key$items[key$scale == "Extra"]
extra_items = key$items[key$scale == "Extra"][[1]]
extra_items
data %>% select(sample, all_of(extra_items)) %>%
group_by(sample)
data %>% select(sample, all_of(extra_items)) %>% group_by(sample) %>% nest() %>% mutate(data = map(data, cor, use = "pairwise")) %>% unnest()
test =data %>% select(sample, all_of(extra_items)) %>% group_by(sample) %>% nest() %>% mutate(data = map(data, cor, use = "pairwise")) %>% unnest()
test
setwd("Dropbox (University of Oregon)/Rapid Response Research (R3)/Data Analysis R3/R code Rapid-R3-Website/")
library(here)#
library(stm)#
library(tidytext)#
library(furrr)#
library(ggpubr)#
library(psych)#
library(conflicted)
conflict_prefer("filter", "dplyr")#
conflict_prefer("lag", "dplyr")#
source(here("Scripts/score data.R")) #
source(here("Functions/pomp.R"))#
conflict_prefer("map", "purrr")
open_ended = scored %>%#
  filter(language != "ES") %>%#
  filter(language != "SPA") %>%#
  select(-contains("OPEN.006")) %>%#
  gather("question", "response", contains("OPEN")) %>%#
  mutate(question = str_extract(question, ".$")) %>%#
  filter(!is.na(response)) %>%#
  group_by(question) %>%#
  nest()
processed4 <- textProcessor(open_ended$data[[4]]$response, metadata = open_ended$data[[4]])#
out4 <- prepDocuments(processed4$documents, processed4$vocab, processed4$meta)#
docs4 <- out4$documents#
vocab4 <- out4$vocab#
meta4 <- out4$meta#
#
heldout4 = make.heldout(docs4, vocab4)
set.seed(07342020)#
plan(multiprocess)#
#
many_models4 <- tibble(K = c(2:30)) %>%#
  mutate(topic_model = future_map(K, ~stm(heldout4$documents, heldout4$vocab, K = .,#
                                          verbose = FALSE)))#
#
k_result4 <- many_models4 %>%#
  mutate(exclusivity = map(topic_model, exclusivity),#
         semantic_coherence = map(topic_model, semanticCoherence, heldout4$documents),#
         eval_heldout = map(topic_model, eval.heldout, heldout4$missing),#
         residual = map(topic_model, checkResiduals, heldout4$documents),#
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),#
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),#
         lbound = bound + lfact,#
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))#
#
save(many_models4, k_result4, file = here("../../Data Management R3/R Data/open4_many_models.Rdata"))
processed5 <- textProcessor(open_ended$data[[5]]$response, metadata = open_ended$data[[5]])#
out5 <- prepDocuments(processed5$documents, processed5$vocab, processed5$meta)#
docs5 <- out5$documents#
vocab5 <- out5$vocab#
meta5 <- out5$meta#
#
heldout5 = make.heldout(docs5, vocab5)
processed5 <- textProcessor(open_ended$data[[5]]$response, metadata = open_ended$data[[5]])#
out5 <- prepDocuments(processed5$documents, processed5$vocab, processed5$meta)#
docs5 <- out5$documents#
vocab5 <- out5$vocab#
meta5 <- out5$meta#
#
heldout5 = make.heldout(docs5, vocab5)#
#
set.seed(07352020)#
plan(multiprocess)#
#
many_models5 <- tibble(K = c(2:30)) %>%#
  mutate(topic_model = future_map(K, ~stm(heldout5$documents, heldout5$vocab, K = .,#
                                          verbose = FALSE)))#
k_result5 <- many_models5 %>%#
  mutate(exclusivity = map(topic_model, exclusivity),#
         semantic_coherence = map(topic_model, semanticCoherence, heldout5$documents),#
         eval_heldout = map(topic_model, eval.heldout, heldout5$missing),#
         residual = map(topic_model, checkResiduals, heldout5$documents),#
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),#
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),#
         lbound = bound + lfact,#
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))#
#
save(many_models5, k_result5, file = here("../../Data Management R3/R Data/open5_many_models.Rdata"))
